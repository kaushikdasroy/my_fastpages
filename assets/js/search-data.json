{
  
    
        "post0": {
            "title": "Install MLFlow Tracking on AWS",
            "content": "There are many MLOps tools ranging from all-in-one to fit for a purpose. Lets take a look at MLFlow which falls in the category of all in one tool having modules supports - Tracking, Projects, Model and Registry. It is an open-source tool. MLFlow supports many machine learning frameworks, such as Tensorflow, PyTorch, XGBoos, H20.ai, Scikit-learn etc. . MLFlow can be used in local machine, and cloud environments. First we will install MLFlow on a AWS environment. This is an experimental setup not a product onw. . In this post I will setup MLFlow Tracking. I will write other posts for other modules of MLFlow. . MLFlow Tracking . Quoting from MLFlow.org “The MLflow Tracking component is an API and UI for logging parameters, code versions, metrics, and output files when running your machine learning code and for later visualizing the results. MLflow Tracking lets you log and query experiments using Python, REST, R API, and Java API APIs.” . MLFlow tracking is organized around runs. Each runexecute a piece of data science code and records some information like - code version, time, source, parameters, Metrics, Artifacts. . runs can be recorded using MLFlow Python, R, Java, REST APIs from anywhere the code is run. These can be recorded from a notebook, cloud or standalone programs . runs can be recorded from a MLFlow Project and MLFlow remembers the project URI. . runs can be organiged in MLFlow experiments. . runs are recorded in the local machine in a folder called mlruns. mlflow ui brings the log in the tracking server for display. . Run and Artifact recording . We will using mlflow with remote tracking server, backend and artifact stores. . In our example we will be using a remote server as tracking server, a Postgresql db as MLFlow entity store and a S3 bucket as our MLFlow artifact store. . Even though it is not demonstrated here, we can record runs by calling mlflow functions, python API in any code we run. The functions are detailed in the mlflow official documentation. Autologging is also supported for most of the frameworks. . All the tracking ui functions can be called programmatically, which makes it easy to log runs and see results in tracking UI. mlflow.set_tracking_uri() connects to a tracking URI. There are many parameters can be passed to this function to establish authentication etc. MLFLOW_TRACKING_URI environment variable can be used to set the tracking URI. . MLFlow Tracking Server . mlflow server command starts a tracking server. Backend storage and artifact storage details are provided with the command. . mlflow server --backend-store-uri /mnt/persistent-disk --default-artifact-root s3://my-mlflow-bucket/ --host 0.0.0.0 . Both a file based or a database based storage are supported as backend storage. SQLAlchemy database URI is used as database storage indicator. &lt;dialect&gt;+&lt;driver&gt;://&lt;username&gt;:&lt;password&gt;@&lt;host&gt;:&lt;port&gt;/&lt;database&gt; MLflow support mysql, mssql, sqlite and postgresql as database dilect. I am going to use a Postgresql in this post. . To run model registry functionality Database based backend storage is required. . Default artifact storage is provided while creating the server and it can be overwritten during an experiment run if a new artifact location is provided. Artificate location can be a NFS file systems or a S3 compatible storage. I will be using Amazon S3. . MLFlow access the S3 access details from the IAM role, a profile in ~/.aws/credentials, or the environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY depending on which of these are available . Launch EC2 environment, Postgresql and S3 Bucket . Setup Tracking server (EC2) . I am using python 3.7 on a AWS Amazon Linux 2 AMI, t2.micro instance. This will work as tracking server. Ensure port 5000 is open to take HTTP request. Setup EC2 instance security to enable HTTP traffic in 5000 PORT. . Setup a RDS Prostgresql db for entity store. . I am creating the db in the same vpc as the EC2 instance and my instance has access to the db. My db security groups are default vpc security groups and these are sufficient to get access to db. If you choose a db outside ec2 vpc, you would require vpc peering and change in db instance’s route table. . Setup a S3 bucket for artifact storage. . Create a s3 bucket. There are few steps required to make sure ec2 has access to the s3 bucket. Create IAM role with minimum required access to S3. Attach the IAM role to ec2 instance. Verify that ec2 has access to s3 bucket by running aws cli s3 ls command in ec2 instance. It should show your bucket name. . Execute Installation . Run following commands: . # update packages sudo yum update # install python sudo yum install python3.7 # install MLFlow and AWS python sdk sudo pip3 install mlflow[extras] psycopg2-binary boto3 # start the mlflow serve. # &quot;nohup is a POSIX command which means &quot;no hang up&quot;. It ignores the HUP signal, does not stop when the user logs out. nohup mlflow server --backend-store-uri postgresql://&lt;USERID&gt;:&lt;PASSWORD&gt;@&lt;DB-ENDPOINT&gt;:5432 --default-artifact-root s3://&lt;S3-BUCKET-NAME&gt; --host 0.0.0.0 &amp; . It will start the MLFlow server . Connect with the MLFow UI . Start the MLFlow ui by accessing the ec2 endpoint port 5000 http://&lt;ec2-endpoint&gt;:5000 . . Reference . Refer to MLFlow official documentation to make the installation production grade by adding more security and learn about how to use the tracking server to track your ML Experiments from different environments. .",
            "url": "https://blog.uplandr.com/mlops/mlflow/2022/01/30/install-mlflow-on-aws.html",
            "relUrl": "/mlops/mlflow/2022/01/30/install-mlflow-on-aws.html",
            "date": " • Jan 30, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Create a simple image processing app using MONAI Deploy SDK",
            "content": "I am going to recreate MONAI official version of imaging processing app creation process given here . I will create a MAP (docker image) of the application and will retain it for reference in application deployment using MONAI Inference Service. . Create MONAI operators and Application Class . Setup Environment . Ensure MONAI Deploy SDK and scikit-image is installed . python -c &quot;import PIL&quot; || pip install -q &quot;Pillow&quot; python -c &quot;import skimage&quot; || pip install -q &quot;scikit-image&quot; python -c &quot;import monai.deploy&quot; || pip install -q &quot;monai-deploy-app-sdk&quot; . . Clone the git repo with code and test files . Note: Case courtesy of Dr Bruno Di Muzio, Radiopaedia.org. From the case rID: 41113 . Clone the MONAI SDK Deploy repository for acessing the test image and example codes. . git clone https://github.com/Project-MONAI/monai-deploy-app-sdk.git . Execute the application code from the cloned repository with path for test input image and location of output. The application code serially strings three operators in a single calss. Three operators are - Sobel Operator, Median Operator and Gaussian Operator. . python examples/apps/simple_imaging_app/app.py -i examples/apps/simple_imaging_app/brain_mr_input.jpg -o output . Note: the above command is same as monai-deply exec command . . Create MONAI App package (MAP Docker Image) . monai-deploy package examples/apps/simple_imaging_app -t simple_app:latest . . Run the docker image with an input image locally . We will use the same input image which we used for test, which is not ideal. . mkdir -p input &amp;&amp; rm -rf input/* cp examples/apps/simple_imaging_app/brain_mr_input.jpg input/ . Execute MAP locally by MAR (MONAI Application Run) . monai-deploy run simple_app:latest input output . . Navigate to the output folder and locate final_output.png . execute eog to see the final_output.png . . We used MONAI Deploy SDK to process a simple image using three operators (Sobel Operator, Median Operator and Gaussian Operator) .",
            "url": "https://blog.uplandr.com/monai%20for%20healthcare/2022/01/29/creating-simple-app-using-monai-deploy.html",
            "relUrl": "/monai%20for%20healthcare/2022/01/29/creating-simple-app-using-monai-deploy.html",
            "date": " • Jan 29, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Setup MONAI Deploy SDK for medical imaging on a AWS Ubuntu",
            "content": "Late 2021 NVIDIA Clara Deploy for traditional medical image inference has been depricated. MONAI Deploy provides the same service as Clara deploy. . Clara deploy is now merged with Clara Holoscan and in the future may have supprt for traditional image devices but for now supporting only medical devices that combines hardware systems and sensors. . Setup MONAI Deploy SDK for medical imaging on a AWS Ubuntu . MONAI Deploy SDK requires Python 3.7 or up. Install or update Python. Ensure pip is installed for the Python version installed. . python3.7 -m pip install pip . . Install MONAI SDK . Now install MONAI Deply SDK with pip install . pip install monai-deploy-app-sdk . . You will have MONAI deploy installed in your server. .",
            "url": "https://blog.uplandr.com/monai%20for%20healthcare/2022/01/27/install-monai-deploy.html",
            "relUrl": "/monai%20for%20healthcare/2022/01/27/install-monai-deploy.html",
            "date": " • Jan 27, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Setup MONAI Deploy for medical imaging on a AWS Ubuntu",
            "content": "Setup MONAI Deploy for medical imaging on a AWS Ubuntu . MONAI Deploy SDK requires Python 3.7 or up. Install or update Python. Ensure pip is installed for the Python version installed. . pip install monai-deploy-app-sdk . . Install MONAI SDK .",
            "url": "https://blog.uplandr.com/monai%20for%20healthcare/2022/01/27/Test-Markdown.html",
            "relUrl": "/monai%20for%20healthcare/2022/01/27/Test-Markdown.html",
            "date": " • Jan 27, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Nvidia Clara based inference using a Fine-tuned model",
            "content": "Inference using Fine-tuned AI Model and NVIDIA Triton Server . Let’s start clara back up again. . . Deployed kubernetes pods . . Setting up TRITON inference server to host our model . Create a folder structure as below . . Move the refined model we created before to this directory. . . Refer: https://blog.uplandr.com/2021/09/02/Fine-tune-a-Chest-Xray-Classification-Model-using-NVIDIA-Clara-Train.html . Refer to this documentation to know about the directory structure: https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/ . Create a file as below: . . Create another file with our labels . . Create a Clara Deploy Operator . We will create a clara deploy operator. This operator will be running in a container independent of clara deploy and the operator can be made part of a deployment pipeline. . Steps are given in here - https://ngc.nvidia.com/catalog/containers/nvidia:clara:app_base_inference . We need to grab these- . Clara deploy base inference operator . | Clara chest classification operator . | TRITIS (Triton) container . | . Make sure you have your ngc connection or else rebuild connection to ngc with docker login nvcr.io . . . . Retag the docker image as latest . . Create a Operator directory structure . . Run the chest xray operator docker container . . Copy 2 files from the container . . . Exit from the container and change the owner for the files to your own. There are few changes to be made in these two files. Change the model to be used to “classification_covidxray_v1” from “classification_cheastxray_v1”. And in the config_inference change the `subtrahend` and `divisor` to 128. . . Create a Dockerfile with base as app_base_inference and copy the config files taken from the chestxray . . Test the custom operator . We will run the operator outside of clara deploy pipeline using docker and a script. . Copy the script from the “executing with docker” section of the link - https://ngc.nvidia.com/catalog/containers/nvidia:clara:app_base_inference . Change the script as follows to make it suitable for our purpose. . Create a file . vi /etc/clara/operators/run_covid_docker.sh . Open the file run_covid_docker.sh and paste the script from “executing with docker” section of the link - https://ngc.nvidia.com/catalog/containers/nvidia:clara:app_base_inference . Need to make following edits: . Replace APP_NAME with “app_covidxray” . Replace MODEL_NAME with “classification_covidxray_v1”. . The line that starts with nvidia-docker — replace $(pwd) with clara/common (so this part reads -v /clara/common/models/${MODEL_NAME}:/models/${MODEL_NAME} . In the line “-v $(pwd)/input:/input ”, replace $(pwd) with “/etc/clara/operators/app_covidxray” . In the line “-v $(pwd)/output:/output ”, replace $(pwd) with “/etc/clara/operators/app_covidxray” . In the line “-v $(pwd)/logs:/logs ”, replace $(pwd) with “/etc/clara/operators/app_covidxray” . In the line “-v $(pwd)/publish:/publish ”, replace $(pwd) with “/etc/clara/operators/app_covidxray” . Comment the lines as indicated in notes of the file if using NGC containers for testing. . Save and exit from the file. . Copy one image in our test input folder. . cp /etc/clara/experiments/covid-training-set/training-images/1-s2.0-S0929664620300449-gr2_lrg-b.png /etc/clara/operators/app_covidxray/input . Change permission of the script file and run the script . chmod 700 /etc/clara/operators/run_covid_docker.sh . cd /etc/clara/operators/ . . To check the job was successful, check the output folder for a file with the inference . . Check the output folder and display the image with labels and categories and % of chance . . Output with inference shown in the picture! . . Create a Clara Deploy Pipeline for inference . Create a clean docker build using the Dockerfile . . The steps are described here - https://docs.nvidia.com/clara/deploy/sdk/Applications/Pipelines/ChestxrayPipeline/public/docs/README.html . https://ngc.nvidia.com/catalog/containers/nvidia:clara:app_base_inference . Start with a chest xray classification pipeline and change it to fit covid xray pipeline . . . Make some changes covidxray-pipeline.yaml file to fit it for our purpose . Change the container image to - app_covidxray, and tag to latest . Remove the pull secrets part . Change all reference of chest xray to covid xray . Note: Make sure the triton server version is appropriate. Pay attention to app_base_inference version, reference pipeline version (in this case clara_ai_chestxray_pipeline) and triton server version. All these need to be in sync for the inference to work. . For the current example I am using app_base_inference ( not app_base_inference_v2 ) and I used nvcr.io/nvidia/tensorrtserver tag 19.08-py3 (rather than tritonserver). Change the “Command” to “trtserver” if using tensorrtserver. . Save and exit . Now we are ready to create our covid xray pipeline . . This will give you a pipeline id. . Run test image through the pipeline . Now use the created pipeline to process one image from the input file. . . Manually start the job . . The completed pipeline view in Clara console (port 32002) . . . Output after download . . Here you have it, your own model is used in inference through triton server and clara pipeline! .",
            "url": "https://blog.uplandr.com/nvidia%20clara%20for%20healthcare/2022/01/20/clara-inference-with-finetuned-model.html",
            "relUrl": "/nvidia%20clara%20for%20healthcare/2022/01/20/clara-inference-with-finetuned-model.html",
            "date": " • Jan 20, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Chest Xray Inference Using A Finetuned Model On Nvidia Clara Triton Inference Server",
            "content": "Inference using AI Model . Let’s start clara back up again. . . Deployed kubernetes pods . . Setting up TRITON inference server to host our model . Create a folder structure as below . . Move the refined model we created before to this directory. . . Refer: https://blog.uplandr.com/2021/09/02/Fine-tune-a-Chest-Xray-Classification-Model-using-NVIDIA-Clara-Train.html . Refer to this documentation to know about the directory structure: https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/ . Create a file as below: . . Create another file with our labels . . Create a Clara Deploy Operator . We will create a clara deploy operator. This operator will be running in a container independent of clara deploy and the operator can be made part of a deployment pipeline. . Steps are given in here - https://ngc.nvidia.com/catalog/containers/nvidia:clara:app_base_inference . We need to grab these- . Clara deploy base inference operator . | Clara chest classification operator . | TRITIS (Triton) container . | . Make sure you have your ngc connection or else rebuild connection to ngc with docker login nvcr.io . . . . Retag the docker image as latest . . Create a Operator directory structure . . Run the chest xray operator docker container . . Copy 2 files from the container . . . Exit from the container and change the owner for the files to your own. There are few changes to be made in these two files. Change the model to be used to “classification_covidxray_v1” from “classification_cheastxray_v1”. And in the config_inference change the `subtrahend` and `divisor` to 128. . . Create a Dockerfile with base as app_base_inference and copy the config files taken from the chestxray . . Test the custom operator . We will run the operator outside of clara deploy pipeline using docker and a script. . Copy the script from the “executing with docker” section of the link - https://ngc.nvidia.com/catalog/containers/nvidia:clara:app_base_inference . Change the script as follows to make it suitable for our purpose. . Create a file . vi /etc/clara/operators/run_covid_docker.sh . Open the file run_covid_docker.sh and paste the script from “executing with docker” section of the link - https://ngc.nvidia.com/catalog/containers/nvidia:clara:app_base_inference . Need to make following edits: . Replace APP_NAME with “app_covidxray” . Replace MODEL_NAME with “classification_covidxray_v1”. . The line that starts with nvidia-docker — replace $(pwd) with clara/common (so this part reads -v /clara/common/models/${MODEL_NAME}:/models/${MODEL_NAME} . In the line “-v $(pwd)/input:/input ”, replace $(pwd) with “/etc/clara/operators/app_covidxray” . In the line “-v $(pwd)/output:/output ”, replace $(pwd) with “/etc/clara/operators/app_covidxray” . In the line “-v $(pwd)/logs:/logs ”, replace $(pwd) with “/etc/clara/operators/app_covidxray” . In the line “-v $(pwd)/publish:/publish ”, replace $(pwd) with “/etc/clara/operators/app_covidxray” . Comment the lines as indicated in notes of the file if using NGC containers for testing. . Save and exit from the file. . Copy one image in our test input folder. . cp /etc/clara/experiments/covid-training-set/training-images/1-s2.0-S0929664620300449-gr2_lrg-b.png /etc/clara/operators/app_covidxray/input . Change permission of the script file and run the script . chmod 700 /etc/clara/operators/run_covid_docker.sh . cd /etc/clara/operators/ . . To check the job was successful, check the output folder for a file with the inference . . Check the output folder and display the image with labels and categories and % of chance . . Output with inference shown in the picture! . . Create a Clara Deploy Pipeline for inference . Create a clean docker build using the Dockerfile . . The steps are described here - https://docs.nvidia.com/clara/deploy/sdk/Applications/Pipelines/ChestxrayPipeline/public/docs/README.html . https://ngc.nvidia.com/catalog/containers/nvidia:clara:app_base_inference . Start with a chest xray classification pipeline and change it to fit covid xray pipeline . . . Make some changes covidxray-pipeline.yaml file to fit it for our purpose . Change the container image to - app_covidxray, and tag to latest . Remove the pull secrets part . Change all reference of chest xray to covid xray . Note: Make sure the triton server version is appropriate. Pay attention to app_base_inference version, reference pipeline version (in this case clara_ai_chestxray_pipeline) and triton server version. All these need to be in sync for the inference to work. . For the current example I am using app_base_inference ( not app_base_inference_v2 ) and I used nvcr.io/nvidia/tensorrtserver tag 19.08-py3 (rather than tritonserver). Change the “Command” to “trtserver” if using tensorrtserver. . Save and exit . Now we are ready to create our covid xray pipeline . . This will give you a pipeline id. . Run test image through the pipeline . Now use the created pipeline to process one image from the input file. . . Manually start the job . . The completed pipeline view in Clara console (port 32002) . . . Output after download . . Here you have it, your own model is used in inference through triton server and clara pipeline! .",
            "url": "https://blog.uplandr.com/nvidia%20clara%20for%20healthcare/2022/01/19/Chest-Xray-Inference-using-a-Finetuned-Model-on-NVIDIA-Clara-Triton-Inference-Server.html",
            "relUrl": "/nvidia%20clara%20for%20healthcare/2022/01/19/Chest-Xray-Inference-using-a-Finetuned-Model-on-NVIDIA-Clara-Triton-Inference-Server.html",
            "date": " • Jan 19, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Pacs Based Covid 19 Classification Ai Workflow",
            "content": "Covid-19 classification pipeline . This is not for diagnostic use and no clinical decision should be made using the contents shared in this post. The model used for Covid-19 classification is not FDA approved for clinical use. . The purpose of this post is purely educational for the understanding of how an end-end clinical workflow can be produced using various components. . The full cycle shown here is as follows: . PACS (Orthanc) initiates an covid-19 classification request to NVIDIA Clara which does the inference and sends back the results back to PACS. Additionally, 3DSlicer is integrated with PACS to extract results for visualization purposes. . This flow is going to use following components: . 3DSlicer for visualization (www.slicer.org ) . | Orthanc DICOM server ( https://www.orthanc-server.com/ ) . | Nvidia Clara Deploy SDK (www.nvidia.com ) . | Nvidia Clara classification model (www.ngc.nvidia.com ) . | Nvidia Clara covid-19 classification (https://ngc.nvidia.com/catalog/resources/nvidia:clara:clara_ai_covid19_pipeline ) . | . Nvidia clara covid-19 pipeline is using pre-built models to create segmentation and classification output. Go through the pipeline from the above link to have a detailed understanding of the flow. . This post is going to extend the pipeline further to integrate it with an Orthanc PACS. Orthanc PACS is used as a DICOM server. An AI analysis can be triggered from the Orthanc PACS with a DICOM series and after the completion of the pipeline, inferences will be automatically fed back to Orthanc. . A desktop based 3DSlicer tool is integrated with the Orthanc PACS to directly get the inferences from the Orthanc PACS to 3DSlicer for visualization. . Orthanc PACS has Stone viewer which is utilized for reading the pipeline generated observation pdf files - covid_lung_ratio and covid probability. . I am using an AWS g4dn.2xlarge instance with 8 vCPUs and 1 GPU. I am on the Ubuntu 18.04 server. Minimum 6 vCPUs are required to execute this pipeline. . The DICOM series provided by the clara covid-19 classification pipeline is utilized for this post. . 3DSlicer visualization of the input series . . Output of the complete pipeline: . . . I could not open the DICOM SEG Modality file generated by clara-seg-writer operator (refer to https://ngc.nvidia.com/catalog/resources/nvidia:clara:clara_ai_covid19_pipeline ) in 3Dslicer DICOM viewer. I got an error while opening the file (ITK exception info: error in unknown: Could not create IO object for reading file) I need to further investigate the reason. . But the Clara render server (localhost:8080) could open the SEG Modality segmentations. Below two segmentations are created by the pipeline. One is for the lung and the other is for lung lesions. . . . Let’s walkthrough the steps taken to create the outputs. . Step1: Create the Clara Deploy pipeline . Follow the setup process described in the following link . https://ngc.nvidia.com/catalog/resources/nvidia:clara:clara_ai_covid19_pipeline/setup . This will create a pipeline . . Step2: Configure the Clara DICOM Adapter for PACS to Pipeline communication . These references are valuable when using Clara DICOM Adapter - . For setting up the adapter . https://nvidia.github.io/clara-dicom-adapter/setup/setup.html . To create and delete AETitles for clara, sources and destinations use the APIs- . https://nvidia.github.io/clara-dicom-adapter/api/rest/config.html . The adapter will do few things for us- . It will receive DICOM series from PACS (Orthanc) . | It will call the Clara pipeline to initiate the inference process . | It will send the inference results back to PACS (Orthanc) . | . Clara Adapter will work as a DICOM device in this case. Everytime a DICOM device calls another device there are few important parameters exchanged for verification. . A “ Calling AETTitle” - AETitle that initiates the call. . | A “Called AETitle” - AETitle that receives the call. . | IP and Port information . | . A good source of basic understanding in PACS and networking between devices is here - https://www.orthanc-server.com/resources/2015-02-09-emsy-tutorial/index.html . 2.1 Instal Orthanc open source PACS . Install Orthanc in a host machine. I am using the same host machine and using the Orthanc Docker container to install Orthanc. It is straightforward process and explained in - https://book.orthanc-server.com/users/docker.html . Few things to keep in mind- . Run the container in detached mode. Make sure to create a persistent volume of the DICOM database and map it to the local host. Create with restart option as “unless-stopped” to start the docker container automatically after restart of host. Use the orthanc image with plugins. Below Docker run command should help: . docker run -d . -p 4242:4242 -p 8042:8042 . --restart unless-stopped . -v /orthanc/orthanc.json:/etc/orthanc/orthanc.json:ro . -v /orthanc/orthanc-db/:/var/lib/orthanc/db/ . jodogne/orthanc-plugins . I have created a folder /orthanc for storing the orthanc config and the database. These folders are mapped to the Docker container. Orthanc uses /etc/orthanc/orthan.json file as config source which is mapped to host /orthanc/orthanc.json. We will make necessary changes in Orthanc by making changes in the /orthanc/orthanc.json and recreating the docker container. . Use port number 8042 to open the Orthanc PACS. Default id/pwd is orthanc/orthanc . After logging in, upload a DICOM series using an Orthanc upload user interface. You can use the same series available with the clara model . . After you upload you will find a DICOM series available in the studies . . 2.1 For PACS(Orthanc) to send DICOM series to Clara Adapter: . 2.1.1 Orthanc has AETitle as “ORTHANC” . . At the Clara Adapter side create a source for the Orthanc using the clara CLI command: . clara dicom create src -a ORTHANC -i 10.244.0.1 . For me the Orthanc host ip is 10.244.0.1 and this will create a source name AETitle “ORTHANC” at clara adapter. . You can check the source by calling clara adapter API . . There are 3 sources listed with one for AETitle ORTHANC. . 2.1.2 Next, let’s create another AETitle which will trigger the Clara pipeline job. . For this run the Clara CLI command . clara dicom create aetitle -a COVIDAET pipeline-covid=&lt;PIPELINE-ID&gt; . Here replace the PIPELINE-ID with the pipeline-id created in step-1 above. This will create a AETitle, “COVIDAET”, which, when called, will trigger a pipeline job indicated by the pipeline-id. . You can check this clara AETitle by calling API: . curl –location –request GET ‘http://localhost:5000/api/config/claraaetitle/COVIDAET’ . . 2.1.3 Install DCMTK . Install DCMTK to proxy the DICOM service class user (SCU) and service class provider (SCP) functionality . Refer to this doc for understanding SCP and SCU - . https://www.orthanc-server.com/resources/2015-02-09-emsy-tutorial/index.html . Install DCMTK by executing - . sudo apt-get install dcmtk . If you like you can create a local Dicom source called “MYPACS” in the host machine and issue a storescu command from the folder where input dicom source is stored to trigger the COVIDAET AETitle and associated pipelinejob. . The command you will use is - . storescu -v +sd +r -xb -aet “MYPACS” -aec “COVIDAET” AA.BB.CC.DD 104 ./ . AA.BB.CC.DD is local host ip . This should complete the pipeline job and create an output file in Clara console to download form. . . 2.1.4 Update PACS server to include Clara AETitle (i.e “COVIDAET) . We need to include COVIDAET as known AETitle in Orthanc for the PACS to send a DICOM series to COVIDAET. We already created a source called “ORTHANC” in clara adapter and therefore clara adapter knows how to handle a request from PACS. . We will add a DICOM Modality for in orthanc config file for COVIDAET . . 2.1.5. Run a Storescu to associate source AETitle “ORTHANC” with Clara AETitle “COVIDAET” . Run the following command . storescu -v +sd +r -xb -aet “ORTHANC” -aec “COVIDAET” AA.BB.CC.DD 104 ./ . AA.BB.CC.DD is host ip address, 104 is port . 2.1.6 Run an incomplete loop by sending DICOM series from Orthanc . At this time you will be able to trigger COVIDAET and its associated pipeline job from Orthanc PACS. Select the DICOM study you uploaded and then select send to DICOM modality followed by “orthanc”. This should start a job . . You can check the status in the Clara Console. . . You may also see whether Orthanc has successfully passed the DICOM series to COVIDAET in Orthanc Jobs tab . . If there is something wrong with the process check the kubernetes pods to understand reasons for failures. . . Specially clara-adapter-* and clara-clara-platformapiserver-* will come handy in the debug process . After successful completion of the job in the console, you may download the output payload for offline analysis and read. . . At this time, Clara render server (port 8080) will be updated with the segmentations as well. . Step 3: Configure Clara Adapter for sending inference back to PACS . If you want to send the results back to PACS instead (which is the most typical scenario), we will update the Clara adapter and PACS config to enable that communication. . In the Clara pipeline, there are steps to register results with outside AETitles. . Clara adapter has the default SCU AETitle as “ClaraSCU”. Orthanc PACS has AETitle “ORTHANC” and uses DICOM Port as 4242 . . 3.1 Create a destination AETitle in Clara Adapter for Orthanc PACS . Use following command to create destination . clara dicom create dest -n orthanc -a ORTHANC -i 10.244.0.1 -p 4242 . . The `-n orthanc` is important as the same name will be used in the pipeline yaml file. . Change the pipeline yaml to reflect the Orthanc PACS as destination by indicating (`orthanc`). There are multiple places in the pipeline where the result got registered. . . Changing pipeline yaml will require creation of a new pipeline. . Create a new pipeline . . Delete the existing aetitle called COVIDAET and create a new one with the same name (COVIDAET) using the pipeline id from above . . To delete clara adapter AETitles use the APIs available at https://nvidia.github.io/clara-dicom-adapter/api/rest/config.html . 3.2 Add `ClaraSCU` in Orthanc PACS for Orthanc to accept results from Clara Adapter . Update Orthanc config file and restart orthanc . . To restart docker, stop and delete docker container and start a new container. The data should persist as we are mapping the DICOM database in the host machine. . With this change `ClaraSCU`should appear in the DICOM modalities of Orthanc PACS . . 3.3 Start a job for starting a job from Orthanc PACS and getting the inference back in the PACS (Full cycle) . Dicom study loaded in Orthanc . . Send to DICOM modality - `orthanc` . . This should start a job on Orthanc . . Clara console will show the progress of the job . . `kubectl get pods` will show the working pods . . Job completed . . . Success! The job is complete as shown in the dicom adapter. . . New studies sent back to PACS . . That completes the full cycle of sending DICOM series from PACS to getting the inference back in the PACS. . The Stone web viewer plugin shows the reports . . . I could not get the segmentation to open in the stone viewer, but used the Clara render server (port 8080) to show the segmentation. . . . At this point connections in PACS should look like this . . Step 4: Connecting PACS with 3DSlicer for visualization . Install 3DSlicer in your visualization server, for me it is in my local machine. . For this we need to add your local machine ip and port in Orthanc PACS config dicom modalities. . And we need to initiate a network connection from 3dslicer with Orthanc ip and its dicom port (4242). It should import the study and the inferences in the local 3dslicer database. . . I could not get the inferred segmentation visible in 3dslicer. The DICOM SEG file could not load. But the output segmentation was visible in the Clara render server. .",
            "url": "https://blog.uplandr.com/nvidia%20clara%20for%20healthcare/2021/09/27/PACS-Based-Covid-19-Classification-AI-Workflow.html",
            "relUrl": "/nvidia%20clara%20for%20healthcare/2021/09/27/PACS-Based-Covid-19-Classification-AI-Workflow.html",
            "date": " • Sep 27, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Malaria Classification Pipeline Of Microscopy Slides",
            "content": "Malaria Classification model execution in Nvidia Clara deploy . This is not for clinical use and should NOT be used for diagnostics purposes. . I will be using the Nvidia Clara Deploy to run a Nvidia GPU Cloud(NGC) model for classification of malaria in microscopy slides converted into png files. . The docker container for the model is in https://ngc.nvidia.com/catalog/containers/nvidia:clara:ai-malaria . As per above url, The network architecture used to train this model is based on the 2015 academic publication “Deep Residual Learning for Image Recognition” by He et. al. . I will be following this link to run a clara pipeline to classify microscopy slides. https://ngc.nvidia.com/catalog/resources/nvidia:clara:clara_ai_malaria_pipeline . I will use a clara deploy SDK running on AWS g4dn.xlarge instance. The description of how to install clara deploy on an AWS instance is in another post. . My running clara instance as shown by the running helm charts . . Create a pipeline directory if you don’t have it already . mkdir -p ~/.clara/pipelines . . Make sure you are connected to nvcr.io . . Pull the malaria classification pipeline . . Inspect the directory and unzip the input file to get the input images . . This will create an input directory containing the png images to be classified . . Unzip the model and related files in a common model directory. Create the /clara/common/models directory if you don’t have it created already . . Create a pipeline utilizing the pipeline yaml file given. Inspect the pipeline yaml file, which builds a docker container as given in https://ngc.nvidia.com/catalog/containers/nvidia:clara:ai-malaria . . Create a pipeline job for the pipeline we just created utilizing the input images to be classified as input . clara create jobs -n malaria-test -p 8f315a0453c6416bbca18bdff457ee26 -f ~/.clara/pipelines/clara_ai_malaria_pipeline/input/png . . Start the job . . Check the job status in Clara console using localhost:32002 . . You may download the output to look at the classified images locally. A stamp on the left corner of the classified out images will indicate a true (`T’) or false (`F`) classification for malaria. . . . You may also use Clara download to download the output in a directory . clara download 602d2d58adbe4476918d9fd73daa7768:/operators/ai-app-malaria/* /etc/clara/experiments/covidtest . . . Use `eog` to look at the output inference files . . . There you have it malaria inference using Clara deploy pipeline .",
            "url": "https://blog.uplandr.com/nvidia%20clara%20for%20healthcare/2021/09/16/Malaria-Classification-Pipeline-of-Microscopy-Slides.html",
            "relUrl": "/nvidia%20clara%20for%20healthcare/2021/09/16/Malaria-Classification-Pipeline-of-Microscopy-Slides.html",
            "date": " • Sep 16, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Brain Tumor Segmentation Model Implementation In Clara Deploy",
            "content": "Brain Tumor Segmentation with Clara Deploy Pipeline . We will use the installed Clara deploy SDK (refer to previous posts to install Clara Deploy in AWS) to run a brain tumor segmentation pipeline. We will use the clara ngc segmentation model for this purpose (ref - https://ngc.nvidia.com/catalog/resources/nvidia:clara:clara_ai_brain_tumor_pipeline) . Create a pipeline folder and pull the pipeline from ngc . clara pull pipeline clara_ai_brain_tumor_pipeline . . Unzip the input dicom series . . Unzip the model . . Register the model . clara create model -p segmentation_mri_brain_tumors_br16_t1c2tc_v1/ -t tensorflow . . Create the pipeline . clara create pipeline -p brain-tumor-pipeline-model-repo.yaml . . Create and start the job . clara create jobs -n brain-test -p b8703c88407848a48a58496ef411daee -f ~/.clara/pipelines/clara_ai_brain_tumor_pipeline/dcm . . Check the Clara Console in localhost:32002 and take the output download of the job. . . View the output in 3d-slicer-segmented tumor . . Location of the tumor through rendering . .",
            "url": "https://blog.uplandr.com/nvidia%20clara%20for%20healthcare/2021/09/15/Brain-Tumor-Segmentation-Model-Implementation-in-Clara-Deploy.html",
            "relUrl": "/nvidia%20clara%20for%20healthcare/2021/09/15/Brain-Tumor-Segmentation-Model-Implementation-in-Clara-Deploy.html",
            "date": " • Sep 15, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Clara Deploy Installation Using Ansible",
            "content": "Clara Deploy installation using Ansible in an Ubuntu 18.04 . Here we are going to set up Clara Deploy in Ubuntu 18.04. This setup can be used for creating clara pipeline for various use cases. The ubuntu server will host the Ansible as well, we are not remoting into the server. . For this I will be using following resource from NVIDIA . https://ngc.nvidia.com/catalog/resources/nvidia:clara:clara_ansible . And following resource from ansible . https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html . Create a g4dn ubuntu 18.04 instance in AWS . . My system has python3 installed . Get the clara deploy ansible installation download from . wget –content-disposition https://api.ngc.nvidia.com/v2/resources/nvidia/clara/clara_ansible/versions/0.8.1-2108.1/zip -O clara_ansible_0.8.1-2108.1.zip . . Unzip the files to get the ansible `playbook` folder. Install unzip with `sudo install unzip` if required. . . My system doesn’t have Ansible installed. We can install ansible using `pip`. We can follow this process outlines in https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html . to install Ansible with pip. . My system doesn’t have a `pip` either. Let’s start by installing pip. . $ curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py . . You may have to install python disutils to install additional python utilities like pip. You can do that my sudo apt-get install python3-distutils . Also install python-apt with . sudo apt-get install python3-apt . Then we can install pip . python3 get-pip.py –user . . Go ahead installing ansible . python3 -m pip install –user ansible . . Make sure to add ansible in $PATH . . Start installing NVIDIA drive from playbook . . I changed the hostname to localhost as I am using the local machine for clara deploy . Do a reboot with . Sudo reboot now . Start installing clara with . Ansible-playbook -K clara.yml . . Clara is installed. Check the version to ensure. . .",
            "url": "https://blog.uplandr.com/nvidia%20clara%20for%20healthcare/2021/09/11/Clara-Deploy-Installation-using-Ansible.html",
            "relUrl": "/nvidia%20clara%20for%20healthcare/2021/09/11/Clara-Deploy-Installation-using-Ansible.html",
            "date": " • Sep 11, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Fine Tune A Chest Xray Classification Model Using Nvidia Clara Train",
            "content": "Training with NVIDIA Clara Train . In this section, you will fine tune a pretrained model with your own data. This model then can be used for inference. Disclaimer - This model is not for clinical use and models are not fit for clinical decision making. . For setting up Clara Train SDK refer to - https://blog.uplandr.com/2021/08/29/Setup-NVIDIA-Clara-Train-Medical-Image.html . I am referring to - https://medium.com/@integratorbrad/how-i-built-a-space-to-train-and-infer-on-medical-imaging-ai-models-part-9-26bbaae9ca2f . And few other sources. . Prepare the training data . For this exercise we will be using training images from https://github.com/ieee8023/covid-chestxray-dataset . Download the repository to ~/Downloads folder . . Unzip the file in the experiments folder . . The contents of the folder . . Install few libraries for python coding to cleanup the input images: . . . . . . Create a folder for storing training images . . Installed sublime text 3 editor for source code creation. . https://linuxize.com/post/how-to-install-sublime-text-3-on-ubuntu-18-04/ . . . Start sublime . . Create convert.py python file for processing the images . . . Run the script to process the files . . Next up, create a json file with image and finding. . Taking the metadata.csv file and cleaning it up to create a datalist.json file. . . . In this process removed some training data with biases or with unique findings with very few data points. . Used xl to open the csv and sort/filter/remove etc as required. . Use https://jsonlint.com/ to check the json for correctness. . Replace all the reference of .jpeg and .jpg to .png in the datalist.json file. . . Take one or two of the images out of the json for running an inference on it later . . . Training the model . Training needs GPU resources and stopping clara deploy will help. . . . Get inside Clara Train container. *Correction* use clara-train-sdk:v3:0 in place of v4. The model I am going to download works with V3 (Tensorflow version of clara). From V4 Clara moved to pytorch. . . Download the chest xray model (MMAR) . . As you can the /clara/experiments folders are mapping inside the Clara Train container . . Lets clone the MMAR to host . . Some cleanup . . Run the following in a separate terminal to give permission. Close the terminal after done. . . Change some of the training script in the train_finetune.sh . . Changing 3 data points 1) json source 2) epoch count(to 500) and 3) Learning Rate (to 0.00002) . Change training configuration . . There are 6 things we are going to change- . After: . 1) Changed epoch to 500, 2) learning rate to 2e-5, 3)Update the “subtrahend” and “divisor” parameters from the CenterData transform, in both the “train” and “validate” sections, 4) &amp; 5) Change image_pipeline name to ClassificationKerasImagePipeline and added “sampling” : “automatic” in training (not in validation), 6) computeAUC aligned with 6 categories . Make #3 and #6 changes in the validation configuration file . . Make following changes to environment configuration. Changes are 1) data_root and 2) dataset_json . Lets begin fine-tuning the model! . Start by making the script executable . . *correction* . Datalist.json file should be in this location - /workspace/data/covid-training-set/training-images/ . Else will get this error! . . Few other errors may arise; fix these with data cleanup or delete. . After this, training will start . . . Now the training is complete. . . As you can see the best metrics was at epoch 570 with validation metric of 0.83 . Get inside docker container if you came out . . See tensorboard output . . . Export the model . . . Exit from the docker container and check the model we trained . . Models are listed . . So we have fine-tuned a model that is trained on the data we created! awesome. .",
            "url": "https://blog.uplandr.com/nvidia%20clara%20for%20healthcare/2021/09/02/Fine-tune-a-Chest-Xray-Classification-Model-using-NVIDIA-Clara-Train.html",
            "relUrl": "/nvidia%20clara%20for%20healthcare/2021/09/02/Fine-tune-a-Chest-Xray-Classification-Model-using-NVIDIA-Clara-Train.html",
            "date": " • Sep 2, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Setup Nvidia Clara Deploy For Medical Image Inference",
            "content": "In this post I will go through the process of setting up NVIDIA Clara Deploy on AWS . I am referring to Brad Genereaux’s blog post to create a NVIDIA Clara based system. I will also use the NVIDIA Clara official installation guide and various other posts to install and troubleshoot. . Brad’s blog - https://medium.com/@integratorbrad/how-i-built-a-space-to-train-and-infer-on-medical-imaging-ai-models-part-1-24ec784edb62 . Disclaimer: The example shown here is NOT FOR CLINICAL USE and learning purpose only. The models are not FDA approved and not to be used for clinical decision making. . If you have an environment setup with ubuntu, docker, CUDA, NVIDIA container toolkit, NGC access then start from ‘Setting up inference environment (Clara Deploy)’ section. . AWS Environment setup . Create an AWS instance with following configuration (taken from NVIDIA official guide): . . Ref: https://docs.nvidia.com/clara/deploy/ClaraInstallation.html#installation-on-a-cloud-service-provider-csp . I will create a spot instance for my environment. P3.8xlarge is an expensive environment, by using a spot instance you can reduce the cost significantly, but it will create some interruption based on spot availability. . You might be ok with using some inexpensive GPU instances like g4dn, but for now I am going with NVIDIA suggested instance(p3) . After creating the spot instance, remote into the AWS server - . . Check that you have a CUDA enabled GPU: . . If nothing comes back from lspci command, then update the PCI hardware database of linux by entering `update-pciids` command and rerun the lspci | grep command. | . Check for the CUDA supported version of linux: . . It is a 64-bit system! . Verify that gcc is installed . . Find out the kernel version of the system . . Before installing CUDA, the kernel header and development package of the same kernel version need to be installed. . . Install CUDA by going to this link and selecting right choices: . https://developer.nvidia.com/cuda-downloads?target_os=Linux . . Reboot the system after you are done with the above steps . . Install Docker . Follow the steps outlined in https://docs.docker.com/engine/install/ubuntu/ . . . Add docker’s official GPG Key . curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg –dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg | . Setup stable repository . echo . “deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu . $(lsb_release -cs) stable” | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null | . Update apt package index . sudo apt-get update . Install docker community edition . sudo apt-get install docker-ce=5:19.03.8~3-0~ubuntu-bionic docker-ce-cli=5:19.03.8~3-0~ubuntu-bionic containerd.io . Verify that Docker is installed . sudo docker run hello-world . Add your user id in Docker user group . sudo usermod -aG docker $USER . Reboot . Sudo reboot now . Install NVIDIA container toolkit . Follow the steps outlined here - . https://github.com/NVIDIA/nvidia-docker . https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker . Setup the stable repository and the GPG key- . distribution=$(. /etc/os-release;echo $ID$VERSION_ID) . &amp;&amp; curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - | . &amp;&amp; curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list | . After updating the package list install the nvidia-docker2 . sudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-docker2 . Restart docker demon . sudo systemctl restart docker . Test by running a base CUDA container . sudo docker run –rm –gpus all nvidia/cuda:11.0-base nvidia-smi . . Configuration of NGC access . Login to NGC (https://ngc.nvidia.com/) and generate API Key and execute the following . mkdir /etc/clara/ngc . cd /etc/clara/ngc . wget https://ngc.nvidia.com/downloads/ngccli_cat_linux.zip &amp;&amp; unzip ngccli_cat_linux.zip &amp;&amp; rm ngccli_cat_linux.zip ngc.md5 &amp;&amp; chmod u+x ngc . Add NGC key in ngc config . ./ngc config set . . Config docker to use NGC token . docker login nvcr.io . . Now Ubuntu is loaded with Docker, NVIDIA docker, NVIDIA container toolkit . Setting up inference environment (Clara Deploy) . Disclaimer: The example shown here is NOT FOR CLINICAL USE and learning purpose only. The models are not FDA approved and not to be used for clinical decision making. . Clara Deploy installation guide is available in nvidia official site - https://docs.nvidia.com/clara/deploy/ClaraInstallation.html . . Download the clara deploy SDK . . Unzip the file . . Now install the Clara deploy prerequisites. To do this run the bootstrap.sh. This will install kubernetes, helm etc. and their dependencies. . Navigate to the /etc/clara/bootstrap directory and run bootstrap.sh . . Bootstrap.sh had old reference to helm repo . . Had to change to get.helm.sh . . Note: Helm 2 is now unsupported . Tiller pod installation issue: I faced problems successfully running tiller pod . . Tiller image is being sourced from gcr.io . . But the help images now moved to github container registry (ghcr.io) . As per the below deployment manifest, the image will be sourced from gcr.io . . To make the image available, pulled the image manually from ghcr.io and retagged the image name with gcr.io, Along with that changed the imagePullPolicy to Never (from IfNotPresent). . After these steps rerun the bootstrap.sh and pre-requisite should install successfully. . . . . Some helpful links to solve the image mismatch issue: . https://www.programmerall.com/article/273716355/ . https://giters.com/helm/helm/issues/10011 . https://programming.vip/docs/kunernets-uses-helm-to-install-tiller-trampling-pit.html . https://cynthiachuang.github.io/Install-NVIDIA-Clara-Deploy-SDK/ . Install the Clara Deploy CLI . Information of Clara Deploy CLI - https://ngc.nvidia.com/catalog/resources/nvidia:clara:clara_cli . Run wget - . . Move to /usr/bin . . Unzip and chmod . . Verify the Clara CLI is working . . Adding the NGC API key to Clara CLI: . clara config –key &lt;NGC_API_KEY&gt; –orgteam nvidia/clara . Replace &lt;NGC_API_KEY&gt; with your NGC key. . Check the nvcr.io is connected thru docker: . . Pull the clara deploy platform . When started to pull clara platform, error . . My helm is v2, whereas “pull” is recognized in v3. Helm needs to be upgraded. Upgrading helm to 3.6.3, changing the helm_version and helm_checksum and re-executing bootstrap.sh . . . Pods are running and didn’t get recreated: . . Tiller pod got created using kubernetes helm container image 2.15.2 . . There is a version mismatch between client side and server side helm. We will see if this is a problem as we progress. . For now, the `clara pull platform` works: . . Clara service is up as shown by the `helm ls`. Let’s bring the other services up. . . . . . Let’s check the PODs . . All running! . Running inference engine using local input file . Pull a chest xray pipeline . . Keep the model in a common model directory: . . Create a pipeline for inference using the pipeline yaml file and clara create: . . It will give a pipeline ID. . The pipeline is running: . . Feed a input pic in png format to the pipeline by creating a job using the pipeline_id from previous step: . . Start the job manually with the job_id from previous step: . . Create an output destination directory and Download the output files: . . Two file got created, . . CSV file showing the chances of diseases: . . The second file shows the image: . . . We have used Clara deploy successfully to get an inference from an x-ray image! .",
            "url": "https://blog.uplandr.com/nvidia%20clara%20for%20healthcare/2021/08/30/Setup-NVIDIA-Clara-Deploy-for-Medical-Image-Inference.html",
            "relUrl": "/nvidia%20clara%20for%20healthcare/2021/08/30/Setup-NVIDIA-Clara-Deploy-for-Medical-Image-Inference.html",
            "date": " • Aug 30, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Setup Nvidia Clara Train Medical Image",
            "content": "In this post I will go through the process of setting up NVIDIA Clara Train on AWS . I am referring to Brad Genereaux’s blog post to create a NVIDIA Clara based system. . Brad’s blog - https://medium.com/@integratorbrad/how-i-built-a-space-to-train-and-infer-on-medical-imaging-ai-models-part-1-24ec784edb62 . I will also use the NVIDIA Clara official installation guide and various other posts to install and troubleshoot. . Disclaimer: The example shown here is NOT FOR CLINICAL USE and learning purpose only. The models are not FDA approved and not to be used for clinical decision making. . AWS Environment setup . Create an AWS instance with following configuration (taken from NVIDIA official guide): . . Ref: https://docs.nvidia.com/clara/deploy/ClaraInstallation.html#installation-on-a-cloud-service-provider-csp . I will create a spot instance for my environment. P3.8xlarge is an expensive environment, by using a spot instance you can reduce the cost significantly, but it will create some interruption based on spot availability. . You might be ok with using some inexpensive GPU instances like g4dn, but for now I am going with NVIDIA suggested instance(p3) . After creating the spot instance, remote into the AWS server - . . Check that you have a CUDA enabled GPU: . . If nothing comes back from lspci command, then update the PCI hardware database of linux by entering `update-pciids` command and rerun the lspci | grep command. | . Check for the CUDA supported version of linux: . . It is a 64-bit system! . Verify that gcc is installed . . Find out the kernel version of the system . . Before installing CUDA, the kernel header and development package of the same kernel version need to be installed. . . Install CUDA by going to this link and selecting right choices: . https://developer.nvidia.com/cuda-downloads?target_os=Linux . . Reboot the system after you are done with the above steps . . Install Docker . Follow the steps outlined in https://docs.docker.com/engine/install/ubuntu/ . . . Add docker’s official GPG Key . curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg –dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg | . Setup stable repository . echo . “deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu . $(lsb_release -cs) stable” | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null | . Update apt package index . sudo apt-get update . Install docker community edition . sudo apt-get install docker-ce=5:19.03.8~3-0~ubuntu-bionic docker-ce-cli=5:19.03.8~3-0~ubuntu-bionic containerd.io . Verify that Docker is installed . sudo docker run hello-world . Add your user id in Docker user group . sudo usermod -aG docker $USER . Reboot . Sudo reboot now . Install NVIDIA container toolkit . Follow the steps outlined here - . https://github.com/NVIDIA/nvidia-docker . https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker . Setup the stable repository and the GPG key- . distribution=$(. /etc/os-release;echo $ID$VERSION_ID) . &amp;&amp; curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - | . &amp;&amp; curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list | . After updating the package list install the nvidia-docker2 . sudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-docker2 . Restart docker demon . sudo systemctl restart docker . Test by running a base CUDA container . sudo docker run –rm –gpus all nvidia/cuda:11.0-base nvidia-smi . . Configuration of NGC access . Login to NGC (https://ngc.nvidia.com/) and generate API Key and execute the following . mkdir /etc/clara/ngc . cd /etc/clara/ngc . wget https://ngc.nvidia.com/downloads/ngccli_cat_linux.zip &amp;&amp; unzip ngccli_cat_linux.zip &amp;&amp; rm ngccli_cat_linux.zip ngc.md5 &amp;&amp; chmod u+x ngc . Add NGC key in ngc config . ./ngc config set . . Config docker to use NGC token . docker login nvcr.io . . Now Ubuntu is loaded with Docker, NVIDIA docker, NVIDIA container toolkit . This picture shows the logical architecture of the Clara Train (taken from NVIDIA Clara github link given above). . . Get the docker container for NVIDIA Clara Tarin SDK . I am using the latest version available(v4) . docker pull nvcr.io/nvidia/clara-train-sdk:v4.0 . If you face problems with space, make sure to add and resize your drive. . . Restart docker pull if the pull fails for any other reasons. . Successfully pulled clara train docker image: . . Make a folder for experiments and change the ownership to user ubuntu: . . Go inside the clara train SDK by starting docker container in interactive mode: . . Now you are inside the clara train docker container. . Run this command to get a full list of nvidia medical models. . . Create a folder for our 1st model - Chest xray . . Set the parameters for the chosen model . . Download the model. . . This will download the covid-19 chest xray classification model. . The details of the model available at https://ngc.nvidia.com/catalog/models/nvidia:med:clara_train_covid19_exam_ehr_xray . The description of the model as given in the above link: “Description . The ultimate goal of this model is to predict the likelihood that a person showing up in the emergency room will need supplemental oxygen, which can aid physicians in determining the appropriate level of care for patients, including ICU placement.” . . The model is in MMAR format. https://docs.nvidia.com/clara/clara-train-sdk/pt/mmar.html . This is how the model download directory looks like. . . This has all the model weights, scripts and transforms. . Exploring the directory in a bit more details to see the contents . . There you have it, Clara Train is up and running for use. .",
            "url": "https://blog.uplandr.com/nvidia%20clara%20for%20healthcare/2021/08/29/Setup-NVIDIA-Clara-Train-Medical-Image.html",
            "relUrl": "/nvidia%20clara%20for%20healthcare/2021/08/29/Setup-NVIDIA-Clara-Train-Medical-Image.html",
            "date": " • Aug 29, 2021"
        }
        
    
  
    
  
    
        ,"post14": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://blog.uplandr.com/2020/01/14/test-markdown-post.html",
            "relUrl": "/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://blog.uplandr.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  

  
  

  

  
  

  

  
  

  
  

  

  

  
  

  
      ,"page13": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://blog.uplandr.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}