{
  
    
        "post0": {
            "title": "Setup Nvidia Clara Deploy For Medical Image Inference",
            "content": "In this post I will go through the process of setting up NVIDIA Clara Deploy on AWS . I am referring to Brad Genereaux’s blog post to create a NVIDIA Clara based system. I will also use the NVIDIA Clara official installation guide and various other posts to install and troubleshoot. . Brad’s blog - https://medium.com/@integratorbrad/how-i-built-a-space-to-train-and-infer-on-medical-imaging-ai-models-part-1-24ec784edb62 . Disclaimer: The example shown here is NOT FOR CLINICAL USE and learning purpose only. The models are not FDA approved and not to be used for clinical decision making. . If you have an environment setup with ubuntu, docker, CUDA, NVIDIA container toolkit, NGC access then start from ‘Setting up inference environment (Clara Deploy)’ section. . AWS Environment setup . Create an AWS instance with following configuration (taken from NVIDIA official guide): . . Ref: https://docs.nvidia.com/clara/deploy/ClaraInstallation.html#installation-on-a-cloud-service-provider-csp . I will create a spot instance for my environment. P3.8xlarge is an expensive environment, by using a spot instance you can reduce the cost significantly, but it will create some interruption based on spot availability. . You might be ok with using some inexpensive GPU instances like g4dn, but for now I am going with NVIDIA suggested instance(p3) . After creating the spot instance, remote into the AWS server - . . Check that you have a CUDA enabled GPU: . . If nothing comes back from lspci command, then update the PCI hardware database of linux by entering `update-pciids` command and rerun the lspci | grep command. | . Check for the CUDA supported version of linux: . . It is a 64-bit system! . Verify that gcc is installed . . Find out the kernel version of the system . . Before installing CUDA, the kernel header and development package of the same kernel version need to be installed. . . Install CUDA by going to this link and selecting right choices: . https://developer.nvidia.com/cuda-downloads?target_os=Linux . . Reboot the system after you are done with the above steps . . Install Docker . Follow the steps outlined in https://docs.docker.com/engine/install/ubuntu/ . . . Add docker’s official GPG Key . curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg –dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg | . Setup stable repository . echo . “deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu . $(lsb_release -cs) stable” | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null | . Update apt package index . sudo apt-get update . Install docker community edition . sudo apt-get install docker-ce=5:19.03.8~3-0~ubuntu-bionic docker-ce-cli=5:19.03.8~3-0~ubuntu-bionic containerd.io . Verify that Docker is installed . sudo docker run hello-world . Add your user id in Docker user group . sudo usermod -aG docker $USER . Reboot . Sudo reboot now . Install NVIDIA container toolkit . Follow the steps outlined here - . https://github.com/NVIDIA/nvidia-docker . https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker . Setup the stable repository and the GPG key- . distribution=$(. /etc/os-release;echo $ID$VERSION_ID) . &amp;&amp; curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - | . &amp;&amp; curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list | . After updating the package list install the nvidia-docker2 . sudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-docker2 . Restart docker demon . sudo systemctl restart docker . Test by running a base CUDA container . sudo docker run –rm –gpus all nvidia/cuda:11.0-base nvidia-smi . . Configuration of NGC access . Login to NGC (https://ngc.nvidia.com/) and generate API Key and execute the following . mkdir /etc/clara/ngc . cd /etc/clara/ngc . wget https://ngc.nvidia.com/downloads/ngccli_cat_linux.zip &amp;&amp; unzip ngccli_cat_linux.zip &amp;&amp; rm ngccli_cat_linux.zip ngc.md5 &amp;&amp; chmod u+x ngc . Add NGC key in ngc config . ./ngc config set . . Config docker to use NGC token . docker login nvcr.io . . Now Ubuntu is loaded with Docker, NVIDIA docker, NVIDIA container toolkit . Setting up inference environment (Clara Deploy) . Disclaimer: The example shown here is NOT FOR CLINICAL USE and learning purpose only. The models are not FDA approved and not to be used for clinical decision making. . Clara Deploy installation guide is available in nvidia official site - https://docs.nvidia.com/clara/deploy/ClaraInstallation.html . . Download the clara deploy SDK . . Unzip the file . . Now install the Clara deploy prerequisites. To do this run the bootstrap.sh. This will install kubernetes, helm etc. and their dependencies. . Navigate to the /etc/clara/bootstrap directory and run bootstrap.sh . . Bootstrap.sh had old reference to helm repo . . Had to change to get.helm.sh . . Note: Helm 2 is now unsupported . Tiller pod installation issue: I faced problems successfully running tiller pod . . Tiller image is being sourced from gcr.io . . But the help images now moved to github container registry (ghcr.io) . As per the below deployment manifest, the image will be sourced from gcr.io . . To make the image available, pulled the image manually from ghcr.io and retagged the image name with gcr.io, Along with that changed the imagePullPolicy to Never (from IfNotPresent). . After these steps rerun the bootstrap.sh and pre-requisite should install successfully. . . . . Some helpful links to solve the image mismatch issue: . https://www.programmerall.com/article/273716355/ . https://giters.com/helm/helm/issues/10011 . https://programming.vip/docs/kunernets-uses-helm-to-install-tiller-trampling-pit.html . https://cynthiachuang.github.io/Install-NVIDIA-Clara-Deploy-SDK/ . Install the Clara Deploy CLI . Information of Clara Deploy CLI - https://ngc.nvidia.com/catalog/resources/nvidia:clara:clara_cli . Run wget - . . Move to /usr/bin . . Unzip and chmod . . Verify the Clara CLI is working . . Adding the NGC API key to Clara CLI: . clara config –key &lt;NGC_API_KEY&gt; –orgteam nvidia/clara . Replace &lt;NGC_API_KEY&gt; with your NGC key. . Check the nvcr.io is connected thru docker: . . Pull the clara deploy platform . When started to pull clara platform, error . . My helm is v2, whereas “pull” is recognized in v3. Helm needs to be upgraded. Upgrading helm to 3.6.3, changing the helm_version and helm_checksum and re-executing bootstrap.sh . . . Pods are running and didn’t get recreated: . . Tiller pod got created using kubernetes helm container image 2.15.2 . . There is a version mismatch between client side and server side helm. We will see if this is a problem as we progress. . For now, the `clara pull platform` works: . . Clara service is up as shown by the `helm ls`. Let’s bring the other services up. . . . . . Let’s check the PODs . . All running! . Running inference engine using local input file . Pull a chest xray pipeline . . Keep the model in a common model directory: . . Create a pipeline for inference using the pipeline yaml file and clara create: . . It will give a pipeline ID. . The pipeline is running: . . Feed a input pic in png format to the pipeline by creating a job using the pipeline_id from previous step: . . Start the job manually with the job_id from previous step: . . Create an output destination directory and Download the output files: . . Two file got created, . . CSV file showing the chances of diseases: . . The second file shows the image: . . . We have used Clara deploy successfully to get an inference from an x-ray image! .",
            "url": "https://blog.uplandr.com/2021/08/30/Setup-NVIDIA-Clara-Deploy-for-Medical-Image-Inference.html",
            "relUrl": "/2021/08/30/Setup-NVIDIA-Clara-Deploy-for-Medical-Image-Inference.html",
            "date": " • Aug 30, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Setup Nvidia Clara Train Medical Image",
            "content": "In this post I will go through the process of setting up NVIDIA Clara Train on AWS . I am referring to Brad Genereaux’s blog post to create a NVIDIA Clara based system. . Brad’s blog - https://medium.com/@integratorbrad/how-i-built-a-space-to-train-and-infer-on-medical-imaging-ai-models-part-1-24ec784edb62 . I will also use the NVIDIA Clara official installation guide and various other posts to install and troubleshoot. . Disclaimer: The example shown here is NOT FOR CLINICAL USE and learning purpose only. The models are not FDA approved and not to be used for clinical decision making. . AWS Environment setup . Create an AWS instance with following configuration (taken from NVIDIA official guide): . . Ref: https://docs.nvidia.com/clara/deploy/ClaraInstallation.html#installation-on-a-cloud-service-provider-csp . I will create a spot instance for my environment. P3.8xlarge is an expensive environment, by using a spot instance you can reduce the cost significantly, but it will create some interruption based on spot availability. . You might be ok with using some inexpensive GPU instances like g4dn, but for now I am going with NVIDIA suggested instance(p3) . After creating the spot instance, remote into the AWS server - . . Check that you have a CUDA enabled GPU: . . If nothing comes back from lspci command, then update the PCI hardware database of linux by entering `update-pciids` command and rerun the lspci | grep command. | . Check for the CUDA supported version of linux: . . It is a 64-bit system! . Verify that gcc is installed . . Find out the kernel version of the system . . Before installing CUDA, the kernel header and development package of the same kernel version need to be installed. . . Install CUDA by going to this link and selecting right choices: . https://developer.nvidia.com/cuda-downloads?target_os=Linux . . Reboot the system after you are done with the above steps . . Install Docker . Follow the steps outlined in https://docs.docker.com/engine/install/ubuntu/ . . . Add docker’s official GPG Key . curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg –dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg | . Setup stable repository . echo . “deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu . $(lsb_release -cs) stable” | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null | . Update apt package index . sudo apt-get update . Install docker community edition . sudo apt-get install docker-ce=5:19.03.8~3-0~ubuntu-bionic docker-ce-cli=5:19.03.8~3-0~ubuntu-bionic containerd.io . Verify that Docker is installed . sudo docker run hello-world . Add your user id in Docker user group . sudo usermod -aG docker $USER . Reboot . Sudo reboot now . Install NVIDIA container toolkit . Follow the steps outlined here - . https://github.com/NVIDIA/nvidia-docker . https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker . Setup the stable repository and the GPG key- . distribution=$(. /etc/os-release;echo $ID$VERSION_ID) . &amp;&amp; curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - | . &amp;&amp; curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list | . After updating the package list install the nvidia-docker2 . sudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-docker2 . Restart docker demon . sudo systemctl restart docker . Test by running a base CUDA container . sudo docker run –rm –gpus all nvidia/cuda:11.0-base nvidia-smi . . Configuration of NGC access . Login to NGC (https://ngc.nvidia.com/) and generate API Key and execute the following . mkdir /etc/clara/ngc . cd /etc/clara/ngc . wget https://ngc.nvidia.com/downloads/ngccli_cat_linux.zip &amp;&amp; unzip ngccli_cat_linux.zip &amp;&amp; rm ngccli_cat_linux.zip ngc.md5 &amp;&amp; chmod u+x ngc . Add NGC key in ngc config . ./ngc config set . . Config docker to use NGC token . docker login nvcr.io . . Now Ubuntu is loaded with Docker, NVIDIA docker, NVIDIA container toolkit . This picture shows the logical architecture of the Clara Train (taken from NVIDIA Clara github link given above). . . Get the docker container for NVIDIA Clara Tarin SDK . I am using the latest version available(v4) . docker pull nvcr.io/nvidia/clara-train-sdk:v4.0 . If you face problems with space, make sure to add and resize your drive. . . Restart docker pull if the pull fails for any other reasons. . Successfully pulled clara train docker image: . . Make a folder for experiments and change the ownership to user ubuntu: . . Go inside the clara train SDK by starting docker container in interactive mode: . . Now you are inside the clara train docker container. . Run this command to get a full list of nvidia medical models. . . Create a folder for our 1st model - Chest xray . . Set the parameters for the chosen model . . Download the model. . . This will download the covid-19 chest xray classification model. . The details of the model available at https://ngc.nvidia.com/catalog/models/nvidia:med:clara_train_covid19_exam_ehr_xray . The description of the model as given in the above link: “Description . The ultimate goal of this model is to predict the likelihood that a person showing up in the emergency room will need supplemental oxygen, which can aid physicians in determining the appropriate level of care for patients, including ICU placement.” . . The model is in MMAR format. https://docs.nvidia.com/clara/clara-train-sdk/pt/mmar.html . This is how the model download directory looks like. . . This has all the model weights, scripts and transforms. . Exploring the directory in a bit more details to see the contents . . There you have it, Clara Train is up and running for use. .",
            "url": "https://blog.uplandr.com/2021/08/29/Setup-NVIDIA-Clara-Train-Medical-Image.html",
            "relUrl": "/2021/08/29/Setup-NVIDIA-Clara-Train-Medical-Image.html",
            "date": " • Aug 29, 2021"
        }
        
    
  
    
  
    
  
    
        ,"post4": {
            "title": "Simple RNN",
            "content": "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python # For example, here&#39;s several helpful packages to load import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # Input data files are available in the read-only &quot;../input/&quot; directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using &quot;Save &amp; Run All&quot; # You can also write temporary files to /kaggle/temp/, but they won&#39;t be saved outside of the current session . Import resources and create data . import torch from torch import nn import numpy as np import matplotlib.pyplot as plt %matplotlib inline . plt.figure(figsize=(8,5)) # how many time steps/data pts are in one batch of data seq_length = 20 # generate evenly spaced data pts time_steps = np.linspace(0, np.pi, seq_length + 1) data = np.sin(time_steps) data.resize((seq_length + 1, 1)) # size becomes (seq_length+1, 1), adds an input_size dimension x = data[:-1] # all but the last piece of data y = data[1:] # all but the first # display the data plt.plot(time_steps[1:], x, &#39;r.&#39;, label=&#39;input, x&#39;) # x plt.plot(time_steps[1:], y, &#39;b.&#39;, label=&#39;target, y&#39;) # y plt.legend(loc=&#39;best&#39;) plt.show() . time_steps . array([0. , 0.15707963, 0.31415927, 0.4712389 , 0.62831853, 0.78539816, 0.9424778 , 1.09955743, 1.25663706, 1.41371669, 1.57079633, 1.72787596, 1.88495559, 2.04203522, 2.19911486, 2.35619449, 2.51327412, 2.67035376, 2.82743339, 2.98451302, 3.14159265]) . time_steps.shape . (20,) . np.set_printoptions(suppress=True) data . array([[0. ], [0.15643447], [0.30901699], [0.4539905 ], [0.58778525], [0.70710678], [0.80901699], [0.89100652], [0.95105652], [0.98768834], [1. ], [0.98768834], [0.95105652], [0.89100652], [0.80901699], [0.70710678], [0.58778525], [0.4539905 ], [0.30901699], [0.15643447], [0. ]]) . data.shape . (21, 1) . np.sin(np.pi/2) . 1.0 . data[1:] . array([[0.15643447], [0.30901699], [0.4539905 ], [0.58778525], [0.70710678], [0.80901699], [0.89100652], [0.95105652], [0.98768834], [1. ], [0.98768834], [0.95105652], [0.89100652], [0.80901699], [0.70710678], [0.58778525], [0.4539905 ], [0.30901699], [0.15643447], [0. ]]) . np.set_printoptions(suppress=True) torch.Tensor(data).unsqueeze(1) . tensor([[[0.0000e+00]], [[1.5643e-01]], [[3.0902e-01]], [[4.5399e-01]], [[5.8779e-01]], [[7.0711e-01]], [[8.0902e-01]], [[8.9101e-01]], [[9.5106e-01]], [[9.8769e-01]], [[1.0000e+00]], [[9.8769e-01]], [[9.5106e-01]], [[8.9101e-01]], [[8.0902e-01]], [[7.0711e-01]], [[5.8779e-01]], [[4.5399e-01]], [[3.0902e-01]], [[1.5643e-01]], [[1.2246e-16]]]) . torch.Tensor(data).unsqueeze(0) . tensor([[[0.0000e+00], [1.5643e-01], [3.0902e-01], [4.5399e-01], [5.8779e-01], [7.0711e-01], [8.0902e-01], [8.9101e-01], [9.5106e-01], [9.8769e-01], [1.0000e+00], [9.8769e-01], [9.5106e-01], [8.9101e-01], [8.0902e-01], [7.0711e-01], [5.8779e-01], [4.5399e-01], [3.0902e-01], [1.5643e-01], [1.2246e-16]]]) . . Define the RNN . Next, we define an RNN in PyTorch. We&#39;ll use nn.RNN to create an RNN layer, then we&#39;ll add a last, fully-connected layer to get the output size that we want. An RNN takes in a number of parameters: . input_size - the size of the input | hidden_dim - the number of features in the RNN output and in the hidden state | n_layers - the number of layers that make up the RNN, typically 1-3; greater than 1 means that you&#39;ll create a stacked RNN | batch_first - whether or not the input/output of the RNN will have the batch_size as the first dimension (batch_size, seq_length, hidden_dim) | . Take a look at the RNN documentation to read more about recurrent layers. . torch.Tensor(data).unsqueeze(0).size() . class RNN(nn.Module): def __init__(self, input_size, output_size, hidden_dim, n_layers): super(RNN, self).__init__() self.hidden_dim=hidden_dim # define an RNN with specified parameters # batch_first means that the first dim of the input and output will be the batch_size self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True) # last, fully-connected layer self.fc = nn.Linear(hidden_dim, output_size) def forward(self, x, hidden): # x (batch_size, seq_length, input_size) # hidden (n_layers, batch_size, hidden_dim) # r_out (batch_size, time_step, hidden_size) batch_size = x.size(0) # get RNN outputs r_out, hidden = self.rnn(x, hidden) # shape output to be (batch_size*seq_length, hidden_dim) r_out = r_out.view(-1, self.hidden_dim) # get final output output = self.fc(r_out) return output, hidden . Check the input and output dimensions . As a check that your model is working as expected, test out how it responds to input data. . test_rnn = RNN(input_size=1, output_size=1, hidden_dim=10, n_layers=2) # generate evenly spaced, test data pts time_steps = np.linspace(0, np.pi, seq_length) data = np.sin(time_steps) data.resize((seq_length, 1)) test_input = torch.Tensor(data).unsqueeze(0) # give it a batch_size of 1 as first dimension print(&#39;Input size: &#39;, test_input.size()) # test out rnn sizes test_out, test_h = test_rnn(test_input, None) print(&#39;Output size: &#39;, test_out.size()) print(&#39;Hidden state size: &#39;, test_h.size()) . Input size: torch.Size([1, 20, 1]) Output size: torch.Size([20, 1]) Hidden state size: torch.Size([2, 1, 10]) . . Training the RNN . Next, we&#39;ll instantiate an RNN with some specified hyperparameters. Then train it over a series of steps, and see how it performs. . input_size=1 output_size=1 hidden_dim=32 n_layers=1 # instantiate an RNN rnn = RNN(input_size, output_size, hidden_dim, n_layers) print(rnn) . RNN( (rnn): RNN(1, 32, batch_first=True) (fc): Linear(in_features=32, out_features=1, bias=True) ) . Loss and Optimization . This is a regression problem: can we train an RNN to accurately predict the next data point, given a current data point? . The data points are coordinate values, so to compare a predicted and ground_truth point, we&#39;ll use a regression loss:the mean squared error.* It&#39;s typical to use an Adam optimizer for recurrent models. | . criterion = nn.MSELoss() optimizer = torch.optim.Adam(rnn.parameters(), lr=0.01) . Defining the training function . This function takes in an rnn, a number of steps to train for, and returns a trained rnn. This function is also responsible for displaying the loss and the predictions, every so often. . Hidden State . Pay close attention to the hidden state, here: . Before looping over a batch of training data, the hidden state is initialized | After a new hidden state is generated by the rnn, we get the latest hidden state, and use that as input to the rnn for the following steps | . def train(rnn, n_steps, print_every): # initialize the hidden state hidden = None for batch_i, step in enumerate(range(n_steps)): # defining the training data time_steps = np.linspace(step * np.pi, (step+1)*np.pi, seq_length + 1) data = np.sin(time_steps) data.resize((seq_length + 1, 1)) # input_size=1 x = data[:-1] y = data[1:] # convert data into Tensors x_tensor = torch.Tensor(x).unsqueeze(0) # unsqueeze gives a 1, batch_size dimension y_tensor = torch.Tensor(y) # outputs from the rnn prediction, hidden = rnn(x_tensor, hidden) ## Representing Memory ## # make a new variable for hidden and detach the hidden state from its history # this way, we don&#39;t backpropagate through the entire history hidden = hidden.data # calculate the loss loss = criterion(prediction, y_tensor) # zero gradients optimizer.zero_grad() # perform backprop and update weights loss.backward() optimizer.step() # display loss and predictions if batch_i%print_every == 0: print(&#39;Loss: &#39;, loss.item()) plt.plot(time_steps[1:], x, &#39;r.&#39;) # input plt.plot(time_steps[1:], prediction.data.numpy().flatten(), &#39;b.&#39;) # predictions plt.show() return rnn . n_steps = 75 print_every = 15 trained_rnn = train(rnn, n_steps, print_every) . Loss: 0.4074189066886902 . Loss: 0.02961491048336029 . Loss: 0.006993359420448542 . Loss: 0.0076007782481610775 . Loss: 0.0006659030914306641 . Time-Series Prediction . Time-series prediction can be applied to many tasks. Think about weather forecasting or predicting the ebb and flow of stock market prices. You can even try to generate predictions much further in the future than just one time step! .",
            "url": "https://blog.uplandr.com/2021/06/06/udacity-rnn-time-series-1.html",
            "relUrl": "/2021/06/06/udacity-rnn-time-series-1.html",
            "date": " • Jun 6, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://blog.uplandr.com/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://blog.uplandr.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://blog.uplandr.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}