<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Inference using the Fine-tuned model | Everything Technical</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Inference using the Fine-tuned model" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Inference using the Fine-tuned model." />
<meta property="og:description" content="Inference using the Fine-tuned model." />
<link rel="canonical" href="https://blog.uplandr.com/nvidia%20clara%20for%20healthcare/2022/01/19/test.html" />
<meta property="og:url" content="https://blog.uplandr.com/nvidia%20clara%20for%20healthcare/2022/01/19/test.html" />
<meta property="og:site_name" content="Everything Technical" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-01-19T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"Inference using the Fine-tuned model.","url":"https://blog.uplandr.com/nvidia%20clara%20for%20healthcare/2022/01/19/test.html","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.uplandr.com/nvidia%20clara%20for%20healthcare/2022/01/19/test.html"},"headline":"Inference using the Fine-tuned model","dateModified":"2022-01-19T00:00:00-06:00","datePublished":"2022-01-19T00:00:00-06:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://blog.uplandr.com/feed.xml" title="Everything Technical" /><!-- the google_analytics_id gets auto inserted from the config file -->



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-CMLEB78YCD"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-CMLEB78YCD');
</script>


<link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Everything Technical</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Topics</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Inference using the Fine-tuned model</h1><p class="page-description">Inference using the Fine-tuned model.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-01-19T00:00:00-06:00" itemprop="datePublished">
        Jan 19, 2022
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      3 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#Nvidia Clara for Healthcare">Nvidia Clara for Healthcare</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#inference-using-fine-tuned-ai-model-and-nvidia-triton-server">Inference using Fine-tuned AI Model and NVIDIA Triton Server</a>
<ul>
<li class="toc-entry toc-h2"><a href="#setting-up-triton-inference-server-to-host-our-model">Setting up TRITON inference server to host our model</a></li>
<li class="toc-entry toc-h2"><a href="#create-a-clara-deploy-operator">Create a Clara Deploy Operator</a></li>
<li class="toc-entry toc-h2"><a href="#test-the-custom-operator">Test the custom operator</a></li>
<li class="toc-entry toc-h2"><a href="#create-a-clara-deploy-pipeline-for-inference">Create a Clara Deploy Pipeline for inference</a></li>
<li class="toc-entry toc-h2"><a href="#run-test-image-through-the-pipeline">Run test image through the pipeline</a></li>
</ul>
</li>
</ul><h1 id="inference-using-fine-tuned-ai-model-and-nvidia-triton-server">
<a class="anchor" href="#inference-using-fine-tuned-ai-model-and-nvidia-triton-server" aria-hidden="true"><span class="octicon octicon-link"></span></a>Inference using Fine-tuned AI Model and NVIDIA Triton Server</h1>

<p>Let’s start clara back up again.</p>

<p><img src="/images/test/image1.png" style="width:6.5in;height:2.20833in"></p>

<p>Deployed kubernetes pods</p>

<p><img src="/images/test/image2.png" style="width:6.5in;height:1.15278in"></p>

<h2 id="setting-up-triton-inference-server-to-host-our-model">
<a class="anchor" href="#setting-up-triton-inference-server-to-host-our-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Setting up TRITON inference server to host our model</h2>

<p>Create a folder structure as below</p>

<p><img src="/images/test/image3.png" style="width:6.5in;height:0.27778in"></p>

<p>Move the refined model we created before to this directory.</p>

<p><img src="/images/test/image4.png" style="width:6.5in;height:0.19444in"></p>

<p>Refer: <a href="https://blog.uplandr.com/2021/09/02/Fine-tune-a-Chest-Xray-Classification-Model-using-NVIDIA-Clara-Train.html"><u>https://blog.uplandr.com/2021/09/02/Fine-tune-a-Chest-Xray-Classification-Model-using-NVIDIA-Clara-Train.html</u></a></p>

<p>Refer to this documentation to know about the directory structure: <a href="https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/"><u>https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/</u></a></p>

<p>Create a file as below:</p>

<p><img src="/images/test/image5.png" style="width:6.5in;height:8.66667in"></p>

<p>Create another file with our labels</p>

<p><img src="/images/test/image6.png" style="width:6.5in;height:7.84722in"></p>

<h2 id="create-a-clara-deploy-operator">
<a class="anchor" href="#create-a-clara-deploy-operator" aria-hidden="true"><span class="octicon octicon-link"></span></a>Create a Clara Deploy Operator</h2>

<p>We will create a clara deploy operator. This operator will be running in a container independent of clara deploy and the operator can be made part of a deployment pipeline.</p>

<p>Steps are given in here - <a href="https://ngc.nvidia.com/catalog/containers/nvidia:clara:app_base_inference"><u>https://ngc.nvidia.com/catalog/containers/nvidia:clara:app_base_inference</u></a></p>

<p>We need to grab these-</p>

<ul>
  <li>
    <p>Clara deploy base inference operator</p>
  </li>
  <li>
    <p>Clara chest classification operator</p>
  </li>
  <li>
    <p>TRITIS (Triton) container</p>
  </li>
</ul>

<p>Make sure you have your ngc connection or else rebuild connection to ngc with docker login nvcr.io</p>

<p><img src="/images/test/image7.png" style="width:6.5in;height:2.625in"></p>

<p><img src="/images/test/image8.png" style="width:6.5in;height:2.70833in"></p>

<p><img src="/images/test/image9.png" style="width:6.5in;height:3.45833in"></p>

<p>Retag the docker image as latest</p>

<p><img src="/images/test/image10.png" style="width:6.5in;height:0.18056in"></p>

<p>Create a Operator directory structure</p>

<p><img src="/images/test/image11.png" style="width:6.5in;height:0.68056in"></p>

<p>Run the chest xray operator docker container</p>

<p><img src="/images/test/image12.png" style="width:6.5in;height:0.18056in"></p>

<p>Copy 2 files from the container</p>

<p><img src="/images/test/image13.png" style="width:6.5in;height:0.19444in"></p>

<p><img src="/images/test/image14.png" style="width:6.5in;height:0.18056in"></p>

<p>Exit from the container and change the owner for the files to your own. There are few changes to be made in these two files. Change the model to be used to “classification_covidxray_v1” from “classification_cheastxray_v1”. And in the config_inference change the `subtrahend` and `divisor` to 128.</p>

<p><img src="/images/test/image15.png" style="width:6.5in;height:0.625in"></p>

<p>Create a Dockerfile with base as app_base_inference and copy the config files taken from the chestxray</p>

<p><img src="/images/test/image16.png" style="width:6.5in;height:6.98611in"></p>

<h2 id="test-the-custom-operator">
<a class="anchor" href="#test-the-custom-operator" aria-hidden="true"><span class="octicon octicon-link"></span></a>Test the custom operator</h2>

<p>We will run the operator outside of clara deploy pipeline using docker and a script.</p>

<p>Copy the script from the “executing with docker” section of the link - <a href="https://ngc.nvidia.com/catalog/containers/nvidia:clara:app_base_inference"><u>https://ngc.nvidia.com/catalog/containers/nvidia:clara:app_base_inference</u></a></p>

<p>Change the script as follows to make it suitable for our purpose.</p>

<p>Create a file</p>

<p>vi /etc/clara/operators/run_covid_docker.sh</p>

<p>Open the file run_covid_docker.sh and paste the script from “executing with docker” section of the link - <a href="https://ngc.nvidia.com/catalog/containers/nvidia:clara:app_base_inference"><u>https://ngc.nvidia.com/catalog/containers/nvidia:clara:app_base_inference</u></a></p>

<p>Need to make following edits:</p>

<p>Replace APP_NAME with “app_covidxray”</p>

<p>Replace MODEL_NAME with “classification_covidxray_v1”.</p>

<p>The line that starts with nvidia-docker — replace $(pwd) with clara/common (so this part reads -v /clara/common/models/${MODEL_NAME}:/models/${MODEL_NAME}</p>

<p>In the line “-v $(pwd)/input:/input \”, replace $(pwd) with “/etc/clara/operators/app_covidxray”</p>

<p>In the line “-v $(pwd)/output:/output \”, replace $(pwd) with “/etc/clara/operators/app_covidxray”</p>

<p>In the line “-v $(pwd)/logs:/logs \”, replace $(pwd) with “/etc/clara/operators/app_covidxray”</p>

<p>In the line “-v $(pwd)/publish:/publish \”, replace $(pwd) with “/etc/clara/operators/app_covidxray”</p>

<p>Comment the lines as indicated in notes of the file if using NGC containers for testing.</p>

<p>Save and exit from the file.</p>

<p>Copy one image in our test input folder.</p>

<p>cp /etc/clara/experiments/covid-training-set/training-images/1-s2.0-S0929664620300449-gr2_lrg-b.png /etc/clara/operators/app_covidxray/input</p>

<p>Change permission of the script file and run the script</p>

<p>chmod 700 /etc/clara/operators/run_covid_docker.sh</p>

<p>cd /etc/clara/operators/</p>

<p><img src="/images/test/image17.png" style="width:6.5in;height:0.23611in"></p>

<p>To check the job was successful, check the output folder for a file with the inference</p>

<p><img src="/images/test/image18.png" style="width:6.5in;height:0.20833in"></p>

<p>Check the output folder and display the image with labels and categories and % of chance</p>

<p><img src="/images/test/image19.png" style="width:6.5in;height:0.45833in"></p>

<p>Output with inference shown in the picture!</p>

<p><img src="/images/test/image20.png" style="width:6.5in;height:5.19444in"></p>

<h2 id="create-a-clara-deploy-pipeline-for-inference">
<a class="anchor" href="#create-a-clara-deploy-pipeline-for-inference" aria-hidden="true"><span class="octicon octicon-link"></span></a>Create a Clara Deploy Pipeline for inference</h2>

<p>Create a clean docker build using the Dockerfile</p>

<p><img src="/images/test/image21.png" style="width:6.5in;height:1.79167in"></p>

<p>The steps are described here - <a href="https://docs.nvidia.com/clara/deploy/sdk/Applications/Pipelines/ChestxrayPipeline/public/docs/README.html"><u>https://docs.nvidia.com/clara/deploy/sdk/Applications/Pipelines/ChestxrayPipeline/public/docs/README.html</u></a></p>

<p><a href="https://ngc.nvidia.com/catalog/containers/nvidia:clara:app_base_inference"><u>https://ngc.nvidia.com/catalog/containers/nvidia:clara:app_base_inference</u></a></p>

<p>Start with a chest xray classification pipeline and change it to fit covid xray pipeline</p>

<p><img src="/images/test/image22.png" style="width:6.5in;height:0.19444in"></p>

<p><img src="/images/test/image23.png" style="width:6.5in;height:0.19444in"></p>

<p>Make some changes covidxray-pipeline.yaml file to fit it for our purpose</p>

<p>Change the container image to - app_covidxray, and tag to latest</p>

<p>Remove the pull secrets part</p>

<p>Change all reference of chest xray to covid xray</p>

<p>Note: Make sure the triton server version is appropriate. Pay attention to app_base_inference version, reference pipeline version (in this case clara_ai_chestxray_pipeline) and triton server version. All these need to be in sync for the inference to work.</p>

<p>For the current example I am using app_base_inference ( not app_base_inference_v2 ) and I used nvcr.io/nvidia/tensorrtserver tag 19.08-py3 (rather than tritonserver). Change the “Command” to “trtserver” if using tensorrtserver.</p>

<p>Save and exit</p>

<p>Now we are ready to create our covid xray pipeline</p>

<p><img src="/images/test/image24.png" style="width:6.5in;height:0.38889in"></p>

<p>This will give you a pipeline id.</p>

<h2 id="run-test-image-through-the-pipeline">
<a class="anchor" href="#run-test-image-through-the-pipeline" aria-hidden="true"><span class="octicon octicon-link"></span></a>Run test image through the pipeline</h2>

<p>Now use the created pipeline to process one image from the input file.</p>

<p><img src="/images/test/image25.png" style="width:6.5in;height:0.44444in"></p>

<p>Manually start the job</p>

<p><img src="/images/test/image26.png" style="width:6.5in;height:0.44444in"></p>

<p>The completed pipeline view in Clara console (port 32002)</p>

<p><img src="/images/test/image27.png" style="width:6.5in;height:1.63889in"></p>

<p><img src="/images/test/image28.png" style="width:6.5in;height:3.91667in"></p>

<p>Output after download</p>

<p><img src="/images/test/image29.png" style="width:5.46875in;height:5.5in"></p>

<p>Here you have it, your own model is used in inference through triton server and clara pipeline!</p>

  </div><a class="u-url" href="/nvidia%20clara%20for%20healthcare/2022/01/19/test.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <!--
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        -->
      </div>
      <div class="footer-col">
        <p>Amazing technology</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul>
</div>

  </div>

</footer>
</body>

</html>
